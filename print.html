<!DOCTYPE HTML>
<html lang="cn" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AxVisor Architecture Book</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AxVisor Architecture Book</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/arceos-hypervisor/doc" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="axvisor"><a class="header" href="#axvisor">AxVisor</a></h1>
<h2 id="the-unified-modular-hypervisor-based-on-arceos"><a class="header" href="#the-unified-modular-hypervisor-based-on-arceos">The unified modular hypervisor based on <a href="https://github.com/arceos-org/arceos">ArceOS</a>.</a></h2>
<ul>
<li>add virtualization support crates/modules based on ArceOS unikernel</li>
<li>build a modular hypervisor that supports multiple architectures based on the basic OS functions</li>
<li>hope to make the hypervisor as modular as possible and minimize modifications to the arceos kernel code.</li>
</ul>
<h2 id="arceos"><a class="header" href="#arceos"><a href="https://github.com/arceos-org/arceos">ArceOS</a></a></h2>
<ul>
<li>An experimental modular OS in Rust.</li>
<li>basic architecture: <strong>unikernel</strong></li>
<li>modules/crates
<ul>
<li>kernel-dependent modules
<ul>
<li>axtask,axdriver,...</li>
</ul>
</li>
<li>Kernel-independent crates
<ul>
<li>buddy allocator, page_table...</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="./assets/arceos.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="快速上手指南"><a class="header" href="#快速上手指南">快速上手指南</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="axvisor-1"><a class="header" href="#axvisor-1">AxVisor</a></h1>
<p>Let's build a hypervisor (Virtual Machine Minotor) upon <a href="https://github.com/arceos-org/arceos">ArceOS</a> unikernel!</p>
<!-- Overall architecture overview can be found [here](doc/README.md). -->
<!-- Refer to these [discussions](https://github.com/arceos-hypervisor/arceos-umhv/discussions) to gain insights into the thoughts and future development directions of this project. -->
<h2 id="preparation"><a class="header" href="#preparation">Preparation</a></h2>
<p>Install <a href="https://github.com/rust-embedded/cargo-binutils">cargo-binutils</a> to use <code>rust-objcopy</code> and <code>rust-objdump</code> tools:</p>
<pre><code class="language-console">$ cargo install cargo-binutils
</code></pre>
<p>Your also need to install <a href="http://musl.cc/x86_64-linux-musl-cross.tgz">musl-gcc</a> to build guest user applications.</p>
<h2 id="guest-vm"><a class="header" href="#guest-vm">Guest VM</a></h2>
<h3 id="configuration-files"><a class="header" href="#configuration-files">Configuration files</a></h3>
<p>Since guest VM configuration is a complex process, ArceOS-Hypervisor chooses to use toml files to manage guest VM configuration,
including vm id, vm name, vm type, number of CPU cores, memory size, virtual devices and pass-through devices, etc.</p>
<p>We provide several configuration file <a href="start/arceos-vmm/configs">templates</a> for setting up guest VMs.</p>
<p>These configuration files are read and parsed by the <code>init_guest_vms()</code> in the <a href="start/arceos-vmm/src/vmm/config.rs">vmm/config</a> mod, and are used to configure the guest VMs.</p>
<p>You can also use <a href="https://github.com/arceos-hypervisor/axvmconfig">axvmconfig</a> tool to generate a custom config.toml.</p>
<p>For more information about VM configuration, visit <a href="https://arceos-hypervisor.github.io/axvmconfig/axvmconfig/index.html">axvmconfig</a> for details.</p>
<h3 id="supported-guest-vms"><a class="header" href="#supported-guest-vms"><a href="start/doc/GuestVMs.html">Supported guest VMs</a></a></h3>
<ul>
<li><a href="https://github.com/arceos-org/arceos">ArceOS</a></li>
<li><a href="https://github.com/Starry-OS">Starry-OS</a></li>
<li><a href="https://github.com/equation314/nimbos">NimbOS</a></li>
<li>Linux
<ul>
<li>currently only Linux with passthrough device on aarch64 is tested.</li>
<li>single core: <a href="start/arceos-vmm/configs/linux-qemu-aarch64.toml">config.toml</a> | <a href="start/arceos-vmm/configs/linux-qemu.dts">dts</a></li>
<li>smp: <a href="start/arceos-vmm/configs/linux-qemu-aarch64-smp2.toml">config.toml</a> | <a href="start/arceos-vmm/configs/linux-qemu-smp2.dts">dts</a></li>
</ul>
</li>
</ul>
<h3 id="loading-guest-vm-images"><a class="header" href="#loading-guest-vm-images">Loading Guest VM images</a></h3>
<p>Currently, arceos-hypervisor supports loading guest VM images from arceos' fat file system, or binding guest VM images to hypervisor images through a static compilation manner (<code>include_bytes</code>).</p>
<ul>
<li>load from file system
<ul>
<li>specify <code>image_location="fs"</code> in the <code>config.toml</code> file.</li>
<li><code>kernel_path</code> in <code>config.toml</code> refers to the location of the kernel image in the arceos rootfs (e.g. <code>disk.img</code>).</li>
<li>Note: <code>"fs"</code> feature is required for arceos-umhv, this can be configured via environment variables <code>APP_FEATURES=fs</code>.</li>
</ul>
</li>
<li>load from memory
<ul>
<li>specify <code>image_location="memory"</code> in the <code>config.toml</code> file.</li>
<li><code>kernel_path</code> in <code>config.toml</code> refers to the relative/absolute path of the kernel image in your workspace when compiling arceos-vmm.</li>
<li>Note that the current method of binding guest VM images through static compilation only supports the loading of one guest VM image at most (usually we use this method to start Linux as a guest VM).</li>
</ul>
</li>
</ul>
<h3 id="build-file-system-image"><a class="header" href="#build-file-system-image">Build File System image</a></h3>
<pre><code class="language-console">$ cd arceos-vmm
$ make disk_img
$
$ # Copy guest VM binary image files.
$ mkdir -p tmp
$ sudo mount disk.img tmp
$ sudo cp /PATH/TO/YOUR/GUEST/VM/IMAGE tmp/
$ sudo umount tmp
$
$ # Otherwise, set `image_location = "memory"` in CONFIG/FILE, then set kernel_path
$ # Arceos-VMM will load the image binaries from the first configuration in the VM_CONFIGS.
</code></pre>
<h4 id="build-ubuntu-file-system-image"><a class="header" href="#build-ubuntu-file-system-image">Build Ubuntu File System image</a></h4>
<pre><code class="language-console"># ARCH=(x86_64|aarch64|riscv64)
make ubuntu_img ARCH=aarch64
</code></pre>
<h2 id="build--run-hypervisor"><a class="header" href="#build--run-hypervisor">Build &amp; Run Hypervisor</a></h2>
<h3 id="example-build-commands"><a class="header" href="#example-build-commands">Example build commands</a></h3>
<pre><code class="language-console">$ cd arceos-vmm
# x86_64 for nimbos
# [LOG=warn|info|debug|trace]
$ make ARCH=x86_64 defconfig
$ make ACCEL=y ARCH=x86_64 LOG=info VM_CONFIGS=configs/vms/nimbos-x86_64.toml APP_FEATURES=fs run
# aarch64 for nimbos
$ make ARCH=aarch64 defconfig
$ make ACCEL=n ARCH=aarch64 LOG=info VM_CONFIGS=configs/vms/nimbos-aarch64.toml APP_FEATURES=fs run
# riscv64 for nimbos
$ make ARCH=riscv64 defconfig
$ make ACCEL=n ARCH=riscv64 LOG=info VM_CONFIGS=configs/vms/nimbos-riscv64.toml APP_FEATURES=fs run
# aarch64 for Linux (remember to change `phys-memory-size` in `arceos-vmm/configs/platforms/aarch64-qemu-virt-hv.toml` as `0x2_0000_0000` (8G))
$ make ARCH=aarch64 VM_CONFIGS=configs/vms/linux-qemu-aarch64.toml LOG=debug BUS=mmio NET=y FEATURES=page-alloc-64g MEM=8g run
# aarch64 for Linux SMP=2
$ make ARCH=aarch64 VM_CONFIGS=configs/vms/linux-qemu-aarch64-smp2.toml LOG=debug BUS=mmio NET=y  BLK=y SMP=2 FEATURES=page-alloc-64g MEM=8g run
</code></pre>
<h3 id="demo-output"><a class="header" href="#demo-output">Demo Output</a></h3>
<pre><code class="language-console">$ cd arceos-vmm
$ make ACCEL=y ARCH=x86_64 LOG=warn VM_CONFIGS=configs/nimbos-x86_64.toml APP_FEATURES=fs run
......
Booting from ROM..
Initialize IDT &amp; GDT...

       d8888                            .d88888b.   .d8888b.
      d88888                           d88P" "Y88b d88P  Y88b
     d88P888                           888     888 Y88b.
    d88P 888 888d888  .d8888b  .d88b.  888     888  "Y888b.
   d88P  888 888P"   d88P"    d8P  Y8b 888     888     "Y88b.
  d88P   888 888     888      88888888 888     888       "888
 d8888888888 888     Y88b.    Y8b.     Y88b. .d88P Y88b  d88P
d88P     888 888      "Y8888P  "Y8888   "Y88888P"   "Y8888P"

arch = x86_64
platform = x86_64-qemu-q35
target = x86_64-unknown-none
smp = 1
build_mode = release
log_level = warn

Starting virtualization...
Running guest...

NN   NN  iii               bb        OOOOO    SSSSS
NNN  NN       mm mm mmmm   bb       OO   OO  SS
NN N NN  iii  mmm  mm  mm  bbbbbb   OO   OO   SSSSS
NN  NNN  iii  mmm  mm  mm  bb   bb  OO   OO       SS
NN   NN  iii  mmm  mm  mm  bbbbbb    OOOO0    SSSSS
              ___    ____    ___    ___
             |__ \  / __ \  |__ \  |__ \
             __/ / / / / /  __/ /  __/ /
            / __/ / /_/ /  / __/  / __/
           /____/ \____/  /____/ /____/

arch = x86_64
platform = rvm-guest-x86_64
build_mode = release
log_level = warn

Initializing kernel heap at: [0xffffff800028ed00, 0xffffff800068ed00)
Initializing IDT...
Loading GDT for CPU 0...
Initializing frame allocator at: [PA:0x68f000, PA:0x1000000)
Mapping .text: [0xffffff8000200000, 0xffffff800021b000)
Mapping .rodata: [0xffffff800021b000, 0xffffff8000220000)
Mapping .data: [0xffffff8000220000, 0xffffff800028a000)
Mapping .bss: [0xffffff800028e000, 0xffffff800068f000)
Mapping boot stack: [0xffffff800028a000, 0xffffff800028e000)
Mapping physical memory: [0xffffff800068f000, 0xffffff8001000000)
Mapping MMIO: [0xffffff80fec00000, 0xffffff80fec01000)
Mapping MMIO: [0xffffff80fed00000, 0xffffff80fed01000)
Mapping MMIO: [0xffffff80fee00000, 0xffffff80fee01000)
Initializing drivers...
Initializing Local APIC...
Initializing HPET...
HPET: 100.000000 MHz, 64-bit, 3 timers
Calibrated TSC frequency: 2993.778 MHz
Calibrated LAPIC frequency: 1000.522 MHz
Initializing task manager...
/**** APPS ****
cyclictest
exit
fantastic_text
forktest
forktest2
forktest_simple
forktest_simple_c
forktree
hello_c
hello_world
matrix
sleep
sleep_simple
stack_overflow
thread_simple
user_shell
usertests
yield
**************/
Running tasks...
test kernel task: pid = TaskId(2), arg = 0xdead
test kernel task: pid = TaskId(3), arg = 0xbeef
Rust user shell
&gt;&gt; hello_world
Hello world from user mode program!
Shell: Process 5 exited with code 0
&gt;&gt;
......
</code></pre>
<h3 id="dev-environment-setup"><a class="header" href="#dev-environment-setup">Dev Environment Setup</a></h3>
<pre><code class="language-shell">tool/dev_env.py
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="linux-镜像启动"><a class="header" href="#linux-镜像启动">Linux 镜像启动</a></h1>
<ol>
<li>
<p>编译 linux 镜像，获取 <code>Image</code> 镜像文件。</p>
</li>
<li>
<p>构建文件系统</p>
<pre><code class="language-bash">cd arceos-vmm
# ARCH=(x86_64|aarch64|riscv64)
make ubuntu_img ARCH=aarch64
</code></pre>
</li>
<li>
<p>准备虚拟机配置文件</p>
<pre><code class="language-bash">cp configs/vms/linux-qemu-aarch64.toml tmp/
# 编译设备树
dtc -I dts -O dtb -o tmp/linux-qemu.dtb configs/vms/linux-qemu.dts
</code></pre>
<p>修改 <code>tmp/linux-qemu-aarch64.toml</code> 文件，将 <code>kernel_path</code> 和 <code>dtb_path</code> 修改相应的绝对路径。</p>
</li>
<li>
<p>运行</p>
<pre><code class="language-bash">make ARCH=aarch64 VM_CONFIGS=tmp/arceos-aarch64.toml LOG=debug BUS=mmio NET=y FEATURES=page-alloc-64g MEM=8g run
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arceos-linux-2虚拟机启动"><a class="header" href="#arceos-linux-2虚拟机启动">ArceOS Linux 2虚拟机启动</a></h1>
<ol>
<li>
<p>编译 linux 镜像，获取 <code>Image</code> 镜像文件。</p>
</li>
<li>
<p>编译 <code>ArceOS</code> 的 <code>Helloworld</code> 用例，得到二进制文件。</p>
</li>
<li>
<p>准备虚拟机配置文件</p>
<pre><code class="language-bash">cd arceos-vmm
cp configs/vms/arceos-aarch64.toml tmp/
cp configs/vms/linux-qemu-aarch64-vm2.toml tmp/

# 编译设备树
dtc -I dts -O dtb -o tmp/linux-qemu.dtb configs/vms/linux-qemu.dts
</code></pre>
<p>修改 <code>tmp/linux-qemu-aarch64-vm2.toml</code>、<code>tmp/arceos-aarch64.toml</code> 文件，将 <code>kernel_path</code> 和 <code>dtb_path</code> 修改相应的绝对路径。</p>
</li>
<li>
<p>运行</p>
<pre><code class="language-bash">make ARCH=aarch64 VM_CONFIGS=tmp/arceos-aarch64.toml:tmp/linux-qemu-aarch64.toml LOG=info BUS=mmio NET=y FEATURES=page-alloc-64g MEM=8g SECOND_SERIAL=y SMP=2 run

# 之后修改.axconfig.toml，将smp=1 改为smp=2，再重新运行
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="axvisor-基于-arceos-的统一模块化-hypervisor-设计文档"><a class="header" href="#axvisor-基于-arceos-的统一模块化-hypervisor-设计文档">AxVisor: 基于 ArceOS 的统一模块化 Hypervisor 设计文档</a></h1>
<h2 id="1-设计目标"><a class="header" href="#1-设计目标">1. 设计目标</a></h2>
<p>ArceOS-Hypervisor 是基于 ArceOS unikernel 框架实现的 Hypervisor。其目标是利用 ArceOS 提供的基础操作系统功能作为基础，实现一个统一的模块化 Hypervisor。统一指使用同一套代码同时支持 x86_64、arm(aarch64) 和 RISC-V 三种架构，以最大化复用架构无关代码，简化代码开发和维护成本。模块化指 Hypervisor 的功能被分解为多个模块，每个模块实现一个特定的功能，模块之间通过标准接口进行通信，以实现功能的解耦和复用。</p>
<p>ArceOS 是一个基于 Rust 语言的 unikernel 框架，其设计目标是提供一个高性能、模块化、最小化的操作系统基座。通过在 ArceOS 的基础上添加不同的模块，就可以对应不同的应用场景生成不同的操作系统：在 ArceOS 上直接添加应用程序，就可以生成一个独立的应用程序 unikernel 镜像；在 ArceOS 上添加宏内核模块，就可以生成一个完整的宏内核操作系统；ArceOS-Hypervisor 则在 ArceOS 的基础上添加虚拟化相关模块，从而以最小成本实现一个 Type-1 Hypervisor。</p>
<p><img src="assets/arceos-backbone.png" alt="arceos-architecture" /></p>
<h2 id="2-软件架构"><a class="header" href="#2-软件架构">2. 软件架构</a></h2>
<p>ArceOS-Hypervisor 的软件架构如下图所示，图中每一个框都是一个独立的模块，模块之间通过标准接口进行通信。包括作为基础的 ArceOS 在内，ArceOS-Hypervisor 的软件架构分为五层：</p>
<p><img src="assets/arceos-hypervisor-architecture.png" alt="arceos-hypervisor-architecture" /></p>
<p>AxVisor 整体架构</p>
<center class="half">
    <img src="./assets/arm mode.png" width="200"/><img src="./assets/x86 mode.png" width="200"/>
</center>
<h3 id="21-arceos"><a class="header" href="#21-arceos">2.1. ArceOS</a></h3>
<p>在 ArceOS-Hypervisor 中，ArceOS 作为最底层的基础存在，提供内存管理、任务调度、设备驱动、同步原语等多种基础功能。ArceOS 的模块化设计允许 ArceOS-Hypervisor 灵活选择需要的模块，这不仅缩减了编译的开销和二进制体积，也提高了系统的安全性和可靠性。</p>
<h3 id="22-arceos-vmm-应用程序app"><a class="header" href="#22-arceos-vmm-应用程序app">2.2. ArceOS-VMM 应用程序（App）</a></h3>
<p>ArceOS-VMM 应用程序是整个 ArceOS-Hypervisor 的核心，它作为 ArceOS 上的一个 unikernel 应用程序运行。ArceOS-VMM 应用程序负责管理虚拟机的生命周期，进行创建、销毁、启动、停止等操作，维护虚拟机的配置、状态、资源等信息，同时也负责处理虚拟机之间的隔离与通信，以及虚拟机对硬件资源的申请和访问。</p>
<h3 id="23-axvm-模块"><a class="header" href="#23-axvm-模块">2.3. <code>axvm</code> 模块</a></h3>
<p><code>axvm</code> 模块位于 ArceOS-Hypervisor 的中间层，它定义了虚拟机的数据结构和操作接口，具体实现了虚拟机的创建、销毁、启动、停止等功能。同时，<code>axvm</code> 模块还负责虚拟机内部虚拟 CPU 的创建、销毁、启动、停止等功能，并负责管理虚拟内存、虚拟设备等资源，实现虚拟环境内操作系统和应用程序对各种虚拟资源的访问。</p>
<h3 id="24-axvcpuaxaddrspace-和-axdevice-模块"><a class="header" href="#24-axvcpuaxaddrspace-和-axdevice-模块">2.4. <code>axvcpu</code>、<code>axaddrspace</code> 和 <code>axdevice</code> 模块</a></h3>
<p>在 <code>axvm</code> 模块的下层是 <code>axvcpu</code>、<code>axaddrspace</code> 和 <code>axdevice</code> 三个具体实现的模块，<code>axvcpu</code> 模块负责架构无关虚拟 CPU 的具体实现，<code>axaddrspace</code> 模块负责虚拟内存的具体实现，<code>axdevice</code> 模块负责虚拟设备的具体实现。这三个模块共同构成了虚拟机的基础设施，为虚拟机提供了 CPU、内存、设备等基本资源。</p>
<p><code>axvcpu</code> 模块定义了虚拟 CPU 的数据结构和统一操作接口。尽管各个架构下的虚拟化技术千差万别，但是通过统一的接口，架构之间的差异在 <code>axvcpu</code> 模块中得到了屏蔽，从而允许 <code>axvm</code> 及以上层的模块不受架构的限制，实现架构无关的虚拟机管理，提高了代码的复用性和可移植性。</p>
<p><code>axaddrspace</code> 模块定义了虚拟内存特别是嵌套页表的数据结构。通过复用 ArceOS 的页表等数据结构，实现了架构无关的虚拟内存管理。</p>
<p><code>axdevice</code> 模块定义了虚拟设备统一访问接口，提供了虚拟设备的基本抽象和封装，允许虚拟机通过统一的接口访问不同的虚拟设备，从而实现虚拟机对硬件资源的访问。</p>
<h3 id="25-具体实现模块"><a class="header" href="#25-具体实现模块">2.5. 具体实现模块</a></h3>
<p>基于 <code>axvcpu</code> 和 <code>axdevice</code> 模块，ArceOS-Hypervisor 实现了不同架构下的具体虚拟 CPU 和虚拟设备模块，虚拟 CPU 包括 <code>x86_vcpu</code>、<code>arm_vcpu</code> 和 <code>riscv_vcpu</code>，虚拟设备包括 <code>x86_vlapic</code>、<code>arm_gic</code> 以及正在实现的 <code>virtio_blk</code>、<code>virtio_net</code> 等等。这些模块实现了具体的虚拟化功能，并且通过 <code>axvcpu</code> 和 <code>axdevice</code> 模块提供的统一接口与其它模块进行交互，这使得代码的复用性和可移植性得到了极大的提高。</p>
<!-- ### 2.6. axvisor_api？

是否需要把 `axvisor_api` 提出来。好处可以写潜在与 ArceOS 解耦，提高可移植性的可能性。另外可以随便说说。
-->
<h2 id="3-运行流程"><a class="header" href="#3-运行流程">3. 运行流程</a></h2>
<h3 id="31-虚拟-cpu-调度"><a class="header" href="#31-虚拟-cpu-调度">3.1. 虚拟 CPU 调度</a></h3>
<p>ArceOS-Hypervisor 的执行流程的核心是虚拟 CPU 的调度。在 ArceOS-Hypervisor 中，虚拟 CPU 是虚拟机的基本执行单元，每个虚拟机可以包含一个或多个虚拟 CPU。虚拟 CPU 的调度是通过复用 ArceOS 的任务调度机制实现的，每个虚拟 CPU 作为一个任务，由 ArceOS 的任务调度器进行调度：</p>
<p><img src="assets/vcpu-scheduling-base.png" alt="vcpu scheduling" /></p>
<p>ArceOS-Hypervisor 还支持混合的调度策略。对于不同的虚拟 CPU，可以采用不同的调度策略，例如，对于实时任务，可以将对应的虚拟 CPU 固定在一个物理 CPU 上，独占物理 CPU 的资源，以保证实时任务的响应时间；对于普通任务，则通过调度器进行动态调度，以实现资源的高效利用：</p>
<p><img src="assets/vcpu-scheduling.png" alt="vcpu scheduling" /></p>
<p>未来计划实现：unikernel axtask、宏内核 process 以及 AxVisor vcpu 的统一调度</p>
<p><img src="./assets/axtask.png" alt="" /></p>
<p><img src="./assets/cpu.png" alt="" /></p>
<h3 id="33-二阶段地址翻译"><a class="header" href="#33-二阶段地址翻译">3.3. 二阶段地址翻译</a></h3>
<p><img src="./assets/pt.png" alt="" /></p>
<h3 id="33-vmexit-处理"><a class="header" href="#33-vmexit-处理">3.3. VMExit 处理</a></h3>
<p><img src="assets/vmexit-handling.png" alt="vmexit handling" /></p>
<h3 id="34-虚拟设备实现"><a class="header" href="#34-虚拟设备实现">3.4 虚拟设备实现</a></h3>
<p><img src="./assets/driver-device.png" alt="" /></p>
<h4 id="341-virtio-device"><a class="header" href="#341-virtio-device">3.4.1 Virtio-device</a></h4>
<p><img src="./assets/io.png" alt="" /></p>
<p>AxVisor 实现 virtio-device 后端设备，具体的设备实现通过类似影子进程的设计转发给 Linux 实现</p>
<p><img src="assets/virtio-backend.png" alt="virtio" /></p>
<h3 id="35-影子进程"><a class="header" href="#35-影子进程">3.5. 影子进程</a></h3>
<p>影子进程是一种通过将具体设备直通给虚拟机内的 Linux 等成品操作系统，让其他虚拟机通过虚拟机间通信和共享内存等方式与这个 Linux 进行通信，从而利用 Linux 中的现有驱动程序来实现虚拟设备的一种技术。影子进程技术可以大大减少虚拟机监控器的开发工作量，提高虚拟机监控器的可移植性和可扩展性。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="axvisor-the-unified-modular-hypervisor-based-on-arceos"><a class="header" href="#axvisor-the-unified-modular-hypervisor-based-on-arceos">AxVisor: The unified modular hypervisor based on <a href="https://github.com/arceos-org/arceos">ArceOS</a>.</a></h1>
<p><a href="https://github.com/orgs/arceos-hypervisor/discussions/7">discussion</a></p>
<!-- ## Design Goal -->
<p>This project originated from the <a href="https://github.com/orgs/rcore-os/discussions/13">discussion/13</a> of <a href="https://github.com/rcore-os">rCore-OS</a> community.</p>
<ul>
<li>
<p>add virtualization support crates/modules based on ArceOS unikernel</p>
</li>
<li>
<p>build a modular hypervisor that supports multiple architectures based on the basic OS functions</p>
</li>
<li>
<p>hope to make the hypervisor as modular as possible and minimize modifications to the arceos kernel code.</p>
</li>
</ul>
<h2 id="arceos-1"><a class="header" href="#arceos-1"><a href="https://github.com/arceos-org/arceos">ArceOS</a></a></h2>
<ul>
<li>An experimental modular OS in Rust.</li>
<li>basic architecture: <strong>unikernel</strong></li>
<li>modules/crates
<ul>
<li>kernel-dependent modules
<ul>
<li>axtask,axdriver,...</li>
</ul>
</li>
<li>Kernel-independent crates
<ul>
<li>buddy allocator, page_table...</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="./assets/arceos.png" alt="" /></p>
<h2 id="heterogeneous-expansion-based-on-arceos-kernel-components"><a class="header" href="#heterogeneous-expansion-based-on-arceos-kernel-components">Heterogeneous expansion based on arceos kernel components</a></h2>
<p><img src="./assets/arceos-hypervisor-architecture.png" alt="" /></p>
<ul>
<li>Kernel Backbone: arceos components</li>
<li>Monolithic  kernel extension: <a href="https://github.com/arceos-org/starry-next">starry-next</a>
<ul>
<li>process address space management</li>
<li>process abstraction</li>
<li>syscall support</li>
</ul>
</li>
<li>Hypervisor extension: <a href="https://github.com/arceos-hypervisor/arceos-umhv/">arceos-hypervisor</a>
<ul>
<li>VM address space management</li>
<li>emulated devices (interrupts, serial ports, etc.)</li>
<li>VM exit interface support</li>
</ul>
</li>
</ul>
<h2 id="arceos-umhv"><a class="header" href="#arceos-umhv">arceos-umhv</a></h2>
<p>Unified modular ArceOS hypervisor, mainly composed of the following independent components:</p>
<!-- * [vmm-app](https://github.com/arceos-hypervisor/arceos-umhv/tree/master/arceos-vmm): acts like a VMM (Virtual Machine Monitor)

* [axvm](https://github.com/arceos-hypervisor/arceos-umhv/tree/master/crates/axvm): responsible for **resource management** within each VM

* [axvcpu](https://github.com/arceos-hypervisor/axvcpu): provides CPU virtualization support


* [axdevice](https://github.com/arceos-hypervisor/axdevice): provides device emulation support


* [axaddrspace](https://github.com/arceos-hypervisor/axaddrspace): provides guest VM address space management -->
<ul>
<li>
<p><a href="https://github.com/arceos-hypervisor/arceos-umhv/tree/master/arceos-vmm">vmm-app</a></p>
</li>
<li>
<p><a href="https://github.com/arceos-hypervisor/arceos-umhv/tree/master/crates/axvm">axvm</a></p>
</li>
<li>
<p><a href="https://github.com/arceos-hypervisor/axvcpu">axvcpu</a></p>
</li>
<li>
<p><a href="https://github.com/arceos-hypervisor/axdevice">axdevice</a></p>
</li>
<li>
<p><a href="https://github.com/arceos-hypervisor/axaddrspace">axaddrspace</a></p>
</li>
</ul>
<p><img src="./assets/arceos-hypervisor-architecture.png" alt="" /></p>
<h2 id="components"><a class="header" href="#components">Components</a></h2>
<ul>
<li><a href="https://github.com/arceos-hypervisor/axvcpu">axvcpu</a>: provides CPU virtualization support
<ul>
<li>highly architecture-dependent</li>
<li>stores exception context frame of different architecture</li>
<li>basic scheduling item</li>
<li>arch-specific vcpu implementations need to be separated into separate crates:
<ul>
<li><a href="https://github.com/arceos-hypervisor/arm_vcpu">arm_vcpu</a></li>
<li><a href="https://github.com/arceos-hypervisor/x86_vcpu">x86_vcpu</a></li>
<li><a href="https://github.com/arceos-hypervisor/riscv_vcpu">riscv_vcpu</a></li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="components-1"><a class="header" href="#components-1">Components</a></h2>
<ul>
<li><a href="https://github.com/arceos-hypervisor/axdevice">axdevice</a>: a module of ArceOS, provides device emulation support
<ul>
<li>partially architecture-independent</li>
<li>different emulated device implementations need to be separated into separate crates
<ul>
<li><a href="https://github.com/arceos-hypervisor/x86_vlapic">x86_vlapic</a></li>
<li><a href="https://github.com/arceos-hypervisor/arm_vgic">arm_vgic</a> (v2,v3,v4)</li>
<li>riscv_vplic</li>
<li>virtio-blk</li>
<li>virtio-net</li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="components-2"><a class="header" href="#components-2">Components</a></h2>
<ul>
<li><a href="https://github.com/arceos-hypervisor/axaddrspace">axaddrspace</a>: provides guest VM address space management
<ul>
<li>nested page table implementation for different architectures</li>
<li>maybe combined with process virtual address space management</li>
<li>responsible for managing and mapping the guest VM's second-stage address space (GPA -&gt; HPA)</li>
<li>implemented based on ArceOS crates:
<ul>
<li><a href="https://crates.io/crates/page_table_entry">page_table_entry</a></li>
<li><a href="https://crates.io/crates/page_table_multiarch">page_table_multiarch</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="components-3"><a class="header" href="#components-3">Components</a></h2>
<ul>
<li><a href="https://github.com/arceos-hypervisor/arceos-umhv/tree/master/crates/axvm">axvm</a>: responsible for <strong>resource management</strong> within each VM
<ul>
<li>partially architecture-independent</li>
<li>a instance of guest virtual machine</li>
<li>resources:
<ul>
<li>address space of guest VM</li>
<li>axvcpu list</li>
<li>axdevice list</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="components-4"><a class="header" href="#components-4">Components</a></h2>
<ul>
<li><a href="https://github.com/arceos-hypervisor/arceos-umhv/tree/master/arceos-vmm">vmm-app</a>: acts like a VMM (Virtual Machine Monitor)
<ul>
<li>As an ArceOS unikernel app, directly call arceos functions</li>
<li>completely architecture-independent</li>
<li>responsible for VM management (configuration &amp; runtime)</li>
</ul>
</li>
</ul>
<h2 id="vcpu-scheduling--based-on-axtask"><a class="header" href="#vcpu-scheduling--based-on-axtask">VCpu Scheduling : based on axtask</a></h2>
<!-- axvcpu is just and only reponsible virtualization function support, e.g. enter/exit guest VM through vmlaunch/vmexit.

Since ArceOS already provides axtask for runtime control flow mangement under single privilege level, 
we can reuse its scheduler and evolve with it.

VCpu scheduling upon ArceOS may looks like this: -->
<pre><code class="language-Rust">    for vcpu in vm.vcpu_list() {
        axtask::spawn(|| {
                let curr = axtask::current();
                let vcpu = unsafe { curr.task_ext().vcpu.clone() };
                let vm = unsafe { curr.task_ext().vm.clone() };
                loop {
                    let exit_reason = vcpu.run();
                    match exit_reason {
                        MMIO(emu_ctx) =&gt; vm.handle_device(emu_ctx),
                        HVC(emu_ctx) =&gt; vm.handle_hvc(emu_ctx),
                        EXIT(code) =&gt; axtask::exit(code),
                        ...
                    }
                }
            }
        );
    }
</code></pre>
<!-- * converge all interactions with the guest privilege level within the `vcpu.run()` function, so that a `loop` block can handle all access from guest VM. -->
<h3 id="exception-vm-exit-handling"><a class="header" href="#exception-vm-exit-handling">Exception (VM-Exit) Handling</a></h3>
<p>The vcpu scheduling design mentioned above requires a reasonable exception (VM-Exit) handling framework.</p>
<ul>
<li>x86_64
<ul>
<li>host-state area of the VMCS can flexibly determine the <code>rip</code> value after a VM-Exit occurs</li>
<li>Therefore, we can save the context properly in the host sp during <code>vmlaunch/resume</code>, store the host sp pointer, and pop the context from the host sp in the <code>vmx_exit</code> function when a VM-Exit occurs. All of these operations are performed in vmx mod, which elegantly limits the interaction with the guest VM to the <code>vcpu.run()</code> function.</li>
</ul>
</li>
</ul>
<h3 id="exception-vm-exit-handling-1"><a class="header" href="#exception-vm-exit-handling-1">Exception (VM-Exit) Handling</a></h3>
<p>The vcpu scheduling design mentioned above requires a reasonable exception (VM-Exit) handling framework.</p>
<ul>
<li>
<p>aarch64</p>
<ul>
<li><a href="https://developer.arm.com/documentation/ddi0601/2020-12/AArch64-Registers/VBAR-EL2--Vector-Base-Address-Register--EL2-"><code>VBAR_EL2</code></a> register holds the vector base address</li>
<li>to run arceos in EL2 to support virtualization, we need to make intrusive modifications to arceos's <a href="https://github.com/arceos-org/arceos/tree/main/modules/axhal">axhal module</a>.</li>
<li>save callee saved registers in EL2's stack manually</li>
</ul>
  <!-- * For VM-Entry
      * `arch_vcpu.run()`
      * save callee saved registers (EL2)
      * pass context frame pointer, something like `&vcpu.vm_context_frame`
      * recore VM context from `&vcpu.vm_context_frame`, including
          * status register
          * general register
      * eret
  * For VM-Exit
      * exception handler
      * get pointer of current vcpu's context frame pointer
      * store VM context into `&current_vcpu.vm_context_frame`, including
          * status register
          * general register
      * pop callee saved registers (EL2)
      * back to `arch_vcpu.run()` -->
</li>
</ul>
<h3 id="multilayer-vm-exit-handling"><a class="header" href="#multilayer-vm-exit-handling">Multilayer VM-Exit handling</a></h3>
<p>VM-Exits in x86_64, aarch64 and riscv64 follow the same design logic but share a slightly different implementation.</p>
<ul>
<li>Inner-VCpu handling
<ul>
<li>e.g. under x86_64, some VM-Exit items are architecture specific (<code>CR_ACCESS</code>, <code>CPUID</code>)</li>
</ul>
</li>
<li>Inner-VM handling
<ul>
<li>leaving device emulation related and page-fault related VM-Exits inside axvm</li>
</ul>
</li>
<li>(Outer-VM)vmm-app handling
<ul>
<li>including the handling of hypercalls (handling this within the VMM also seems quite reasonable) and any (if-any) VM-Exit types that require vCPU scheduling or vCPU exit</li>
</ul>
</li>
</ul>
<!-- 

### Exception Type (VM-Exit Reason)

* architecture independent
* reference: [KVM exit reasion](https://docs.rs/kvm-ioctls/0.17.0/kvm_ioctls/enum.VcpuExit.html)
* current implementation [crates/axvm/src/vcpu.rs](https://github.com/arceos-hypervisor/arceos-umhv/blob/master/crates/axvm/src/vcpu.rs) -->
<h2 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h2>
<ul>
<li>
<p>similar to the address space management of the <a href="https://github.com/arceos-org/arceos/tree/monolithickernel-new/">arceos-monolithic</a>.</p>
</li>
<li>
<p>take advantage of crate <a href="https://github.com/arceos-org/arceos/tree/monolithickernel-new/crates/memory_set">memory_set</a> and register the PageTable as our <a href="https://github.com/arceos-hypervisor/arceos-umhv/blob/master/crates/axvm/src/mm/npt.rs">AxNestPageTable</a>.</p>
</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// The virtual memory address space.
pub struct AddrSpace&lt;H: PagingHandler&gt; {
    va_range: GuestPhysAddrRange,
    areas: MemorySet&lt;Backend&lt;H&gt;&gt;,
    pt: PageTable&lt;H&gt;,
}
<span class="boring">}</span></code></pre></pre>
<blockquote>
<p>we hope to find a way to unify address space management for both monolithic and hypervisor variants of ArceOS.</p>
</blockquote>
<h2 id="emulated-device"><a class="header" href="#emulated-device">Emulated Device</a></h2>
<p><code>axdevice</code> crate provides struct like <code>AxEmulatedDevices</code>, which will be owned and managed by <code>AxVM</code>.</p>
<pre><code class="language-Rust">pub struct AxEmulatedDevices {
    mmio_devices: BTreeMap&lt;Range&lt;usize&gt;, dyn EmuDev&gt;,
    #[cfg(target_arch = "x86_64")]
    pio_devices: BTreeMap&lt;Range&lt;usize&gt;, dyn EmuDev&gt;,
}

pub trait EmuDev {
    fn emu_type(&amp;self) -&gt; EmuDeviceType;
    fn address_range(&amp;self) -&gt; Range&lt;usize&gt;;
    fn handler(&amp;self, ctx: &amp;AccessContext) -&gt; AxResult;
}
</code></pre>
<!-- When a VM-Exit caused by MMIO (or PIO) access occurs, `axvcpu` will record the current access information, including address, bit width, read/write, etc. The `emulated_device_handler` function of axdevice needs to find the corresponding emulated device according to the access address, call the corresponding device's processing function and pass in the access information.

Providing emulated device support for guest VMs requires considerable work.
Currently we focus on [emulated interrupt controller](https://github.com/arceos-hypervisor/arceos-hypervisor-docs/blob/master/devices/emulated_interrupt_controller.md) for different architectures and virtio-devices (mainly [Virtio-Blk](https://github.com/arceos-hypervisor/arceos-hypervisor-docs/blob/master/devices/virtio_blk.md)). -->
<h2 id="dependency-diagram"><a class="header" href="#dependency-diagram">Dependency diagram</a></h2>
<p><img src="./assets/arceos-hv-dep.svg" alt="" /></p>
<!-- * Note: we aim to consolidate all dependencies on ArceOS within the vmm-app -->
<p>Since modules/crates used for virtualization functionality in the ArceOS-Hypervisor architecture need to call OS-related resource management interfaces, <strong>while we aim to consolidate all OS-related dependencies within the vmm-app</strong>.</p>
<p>Various modules/crates will achieve dependency injection through Rust traits.</p>
<h2 id="example-about-how-we-achieve-dependency-injection"><a class="header" href="#example-about-how-we-achieve-dependency-injection">Example about how we achieve dependency injection</a></h2>
<p>Taking <a href="https://github.com/arceos-hypervisor/axaddrspace"><code>axaddrspace</code></a> for an example, its <a href="https://github.com/arceos-hypervisor/axaddrspace/blob/d377e5aa4eb06afa50a3a901ec3239559be1eb51/src/address_space.rs#L16C12-L16C21"><code>AddrSpace</code></a> represents memory regions and two-stage address mapping for guest VM, which relies on a generic type <code>PagingHandler</code> for page table related stuff.</p>
<pre><code class="language-Rust">/// The virtual memory address space.
pub struct AddrSpace&lt;H: PagingHandler&gt; {
    va_range: VirtAddrRange,
    areas: MemorySet&lt;MappingFlags, PageTable&lt;H&gt;, Backend&gt;,
    pt: PageTable&lt;H&gt;,
}
</code></pre>
<h2 id="example-about-how-we-achieve-dependency-injection-1"><a class="header" href="#example-about-how-we-achieve-dependency-injection-1">Example about how we achieve dependency injection</a></h2>
<p><code>axaddrspace</code> is owned and managed by <code>axvm</code>'s <code>AxVM</code> structure, which replies on <code>AxVMHal</code> trait ( defined in <code>axvm</code>'s <a href="https://github.com/arceos-hypervisor/axvm/blob/master/src/hal.rs">hal.rs</a> ) .</p>
<p>Indeed, <code>PagingHandler</code> is a associate type of <code>AxVMHal</code> trait.</p>
<pre><code class="language-Rust">/// The interfaces which the underlying software (kernel or hypervisor) must implement.
pub trait AxVMHal: Sized {
    type PagingHandler: page_table_multiarch::PagingHandler;
    /// Converts a virtual address to the corresponding physical address.
    fn virt_to_phys(vaddr: HostVirtAddr) -&gt; HostPhysAddr;
    /// Current time in nanoseconds.
    fn current_time_nanos() -&gt; u64;
	// ...
}
</code></pre>
<h2 id="example-about-how-we-achieve-dependency-injection-2"><a class="header" href="#example-about-how-we-achieve-dependency-injection-2">Example about how we achieve dependency injection</a></h2>
<p>While <code>AxVMHal</code> is implemented by <code>AxVMHalImpl</code> in vmm-app, which rely on <code>PagingHandlerImpl</code> from <code>ArceOS</code>'s <code>axhal</code> module to implement its associate type <code>PagingHandler</code>.</p>
<pre><code class="language-Rust">pub struct AxVMHalImpl;

impl AxVMHal for AxVMHalImpl {
    type PagingHandler = axhal::paging::PagingHandlerImpl;
    fn virt_to_phys(vaddr: VirtAddr) -&gt; PhysAddr {
        axhal::mem::virt_to_phys(vaddr)
    }
    fn current_time_nanos() -&gt; u64 {
        axhal::time::monotonic_time_nanos()
    }
	// ...
}
</code></pre>
<h2 id="dependency-injection"><a class="header" href="#dependency-injection">Dependency injection</a></h2>
<p>So, current design achieve dependency injection through Rust's generic type (<code>Trait</code>) and its associate type mechanism.</p>
<p>For other virtualization-related modules/crates such as <code>axvcpu</code>, <code>axdevice</code>, etc.,
we also want them to expose well-designed generics, and to converge these carefully crafted generics as subtraits or associated types within the <code>AxVmHal trait</code> of <code>axvm</code> (since <code>axvm</code> is reponsible for VM resource management).</p>
<p>Ultimately, the <code>vmm-app</code> layer will call the relevant functionalities of <code>ArceOS</code> to implement them.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arceos-hypervisor-supported-guestvms"><a class="header" href="#arceos-hypervisor-supported-guestvms">ArceOS-Hypervisor Supported GuestVMs</a></h1>
<h2 id="nimbos"><a class="header" href="#nimbos"><a href="https://github.com/arceos-hypervisor/nimbos">NimbOS</a></a></h2>
<ul>
<li>Simple real time guest VM that can only be used for <strong>single-core</strong> testing</li>
<li>It supports the x86_64, aarch64, and riscv64 architectures</li>
<li>Configuration file templates at <a href="../arceos-vmm/configs/nimbos-aarch64.toml">nimbos-aarch64.toml</a>, <a href="../arceos-vmm/configs/nimbos-x86_64.toml">nimbos-x86_64.toml</a>, and <a href="../arceos-vmm/configs/nimbos-riscv64.toml">nimbos-riscv64.toml</a></li>
<li>Kernel binary images availble at <a href="https://github.com/arceos-hypervisor/nimbos/releases/tag/v0.6">nimbos/releases</a></li>
</ul>
<h2 id="arceos-2"><a class="header" href="#arceos-2"><a href="https://github.com/arceos-hypervisor/arceos">ArceOS</a></a></h2>
<ul>
<li>Used for <strong>SMP</strong> testing</li>
<li>It supports the x86_64, aarch64, and riscv64 architectures</li>
<li>Configuration file templates at <a href="../arceos-vmm/configs/arceos-aarch64.toml">arceos-aarch64.toml</a>, <a href="../arceos-vmm/configs/arceos-x86_64.toml">arceos-x86_64.toml</a>, and <a href="../arceos-vmm/configs/arceos-riscv64.toml">arceos-riscv64.toml</a></li>
</ul>
<h3 id="testcases"><a class="header" href="#testcases">Testcases</a></h3>
<ul>
<li>
<p><strong>Hypercall</strong>:</p>
<ul>
<li>ArceOS HelloWorld application that can be used to test hypercall functionality is provided <a href="https://github.com/arceos-hypervisor/arceos/blob/gvm_test/examples/helloworld/src/main.rs">here</a></li>
<li>Just run <code>make A=examples/helloworld ARCH=[x86_64|aarch64|riscv64] build</code> to get binary images</li>
</ul>
</li>
<li>
<p><strong>virtio-pci-devices (PCI)</strong>:</p>
<ul>
<li>Branch (pci_pio)[https://github.com/hky1999/arceos/tree/pci_pio] can be used for virtio-pci devices testing (PCI device probed through port I/O)</li>
</ul>
</li>
</ul>
<h2 id="axvm-bios"><a class="header" href="#axvm-bios"><a href="https://github.com/arceos-hypervisor/axvm-bios-x86">axvm-bios</a></a></h2>
<ul>
<li>A extremely simple bios for x86_64 guests</li>
<li>It can act as a bootloader for NimbOS and ArceOS</li>
<li>Binary product available at <a href="https://github.com/arceos-hypervisor/axvm-bios-x86/releases/download/v0.1/axvm-bios.bin">here</a></li>
</ul>
<h1 id="arceos-hypervisor-in-rk3588-board"><a class="header" href="#arceos-hypervisor-in-rk3588-board">ArceOS-Hypervisor in RK3588 board</a></h1>
<h2 id="how-to-run-arceos-on-rk3588"><a class="header" href="#how-to-run-arceos-on-rk3588">How to run ArceOS on rk3588</a></h2>
<ol>
<li>Prepare your kernal file <code>linux-rk3588-aarch64.bin</code> and DTB file <code>rk3588.dtb</code>.</li>
<li>Set the kernel path and DTB path in the configuration file <code>configs/linux-rk3588-aarch64.toml</code>.
<pre><code class="language-toml">image_location = "memory"
kernel_path = "/path/to/linux-rk3588-aarch64.bin"
dtb_path = "/path/to/rk3588.dtb"
</code></pre>
</li>
<li>Use Command <code>make A=(pwd) ARCH=aarch64 VM_CONFIGS=configs/linux-rk3588-aarch64.toml kernel</code> to build the kernel image <code>boot.img</code>.</li>
<li>Download the <a href="https://download.t-firefly.com/product/Board/RK3588/Tool/Window/RKDevTool_Release_v3.31.zip">RKDevTool</a>.
<blockquote>
<p>This tool has only been tested on <a href="https://www.pji.net.cn/">Pji's</a> Electronic Control Unit of RK3588. Other RK3588 development boards require independent testing.</p>
</blockquote>
</li>
<li>Set the path of <code>boot.img</code> in <strong>boot</strong> and connect the RK3588 board.</li>
<li>Press the <code>Run</code> button to flash the image to the RK3588 board.
<img src="./figures/RKDevTool3.3.png" alt="RKDevTool" /></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="https://github.com/arceos-hypervisor/axvcpu">axvcpu</a>: provides CPU virtualization support
<ul>
<li>highly architecture-dependent</li>
<li>stores exception context frame of different architecture</li>
<li>basic scheduling item</li>
<li>arch-specific vcpu implementations need to be separated into separate crates:
<ul>
<li><a href="https://github.com/arceos-hypervisor/arm_vcpu">arm_vcpu</a></li>
<li><a href="https://github.com/arceos-hypervisor/x86_vcpu">x86_vcpu</a></li>
<li><a href="https://github.com/arceos-hypervisor/riscv_vcpu">riscv_vcpu</a></li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="https://github.com/arceos-hypervisor/x86_vcpu">x86_64</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="https://github.com/arceos-hypervisor/arm_vcpu">aarch64</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="https://github.com/arceos-hypervisor/riscv_vcpu">riscv64</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><p>🚧 Coming soon.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="two-stage-memory-translation"><a class="header" href="#two-stage-memory-translation">Two-stage memory translation</a></h1>
<p><img src="./assets/pt.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><ul>
<li><a href="https://github.com/arceos-hypervisor/axdevice">axdevice</a>: a module of ArceOS, provides device emulation support
<ul>
<li>partially architecture-independent</li>
<li>different emulated device implementations need to be separated into separate crates
<ul>
<li><a href="https://github.com/arceos-hypervisor/x86_vlapic">x86_vlapic</a></li>
<li><a href="https://github.com/arceos-hypervisor/arm_vgic">arm_vgic</a> (v2,v3,v4)</li>
<li>riscv_vplic</li>
<li>virtio-blk</li>
<li>virtio-net</li>
<li>...</li>
</ul>
</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h1 id="virtio-device"><a class="header" href="#virtio-device">Virtio-Device</a></h1>
<p><img src="device/../assets/io.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multilayer-vm-exit-handling-mechanism"><a class="header" href="#multilayer-vm-exit-handling-mechanism"><a href="https://github.com/orgs/arceos-hypervisor/discussions/19">Multilayer VM-Exit handling mechanism</a></a></h1>
<h2 id="vm-exits"><a class="header" href="#vm-exits">VM-Exits</a></h2>
<p>As we all know, VM-Exits are curtial for getting guest VM's running states and interacting with Guest VMs.</p>
<p>VM-Exits are used for device emulation and vCPU scheduling.</p>
<p>VM-Exits in x86_64, aarch64 and riscv64 follow the same design logic but share a slightly different implementation.</p>
<p><img src="designs/../assets/vmexit-handling.png" alt="" /></p>
<h2 id="inner-vcpu-handling"><a class="header" href="#inner-vcpu-handling">Inner-VCpu handling</a></h2>
<p>Under x86_64, some VM-Exit items are architecture specific (e.g. <code>VmxExitReason::CR_ACCESS</code>, <code>VmxExitReason::CPUID</code>).
In our current design, these VM-Exits are handled by [<code>VmxVcpu</code>] itself through <code>builtin_vmexit_handler</code>, while other VM-Exit types are returned by <code>vcpu.run()</code> and leaves whoever called <code>vcpu.run()</code> to handle.</p>
<pre><code class="language-Rust">impl&lt;H: AxVMHal&gt; VmxVcpu&lt;H&gt; {
    /// Handle vm-exits than can and should be handled by [`VmxVcpu`] itself.
    ///
    /// Return the result or None if the vm-exit was not handled.
    fn builtin_vmexit_handler(&amp;mut self, exit_info: &amp;VmxExitInfo) -&gt; Option&lt;AxResult&gt; {
        // Following vm-exits are handled here:
        // - interrupt window: turn off interrupt window;
        // - xsetbv: set guest xcr;
        // - cr access: just panic;
        match exit_info.exit_reason {
            VmxExitReason::INTERRUPT_WINDOW =&gt; Some(self.set_interrupt_window(false)),
            VmxExitReason::PREEMPTION_TIMER =&gt; Some(self.handle_vmx_preemption_timer()),
            VmxExitReason::XSETBV =&gt; Some(self.handle_xsetbv()),
            VmxExitReason::CR_ACCESS =&gt; Some(self.handle_cr()),
            VmxExitReason::CPUID =&gt; Some(self.handle_cpuid()),
            _ =&gt; None,
        }
    }
}
</code></pre>
<p>Besides, <code>VmxExitReason::IoRead/IoWrite</code> and  <code>VmxExitReason::MsrRead/MsrWrite</code> are also x86_64 specific, but these VM-Exits are relavant to Port I/O or Msr device emulation, make them more suitable to be handled outside the <code>vcpu.run()</code>.</p>
<h2 id="inner-vm-handling"><a class="header" href="#inner-vm-handling">Inner-VM handling</a></h2>
<p>Since VM structure in <code>axvm</code> is responsible for VM's resource management like emulated devices and address space (<code>axaddrspace</code>). I prefer leaving device emulation related and page-fault related (data abort) VM-Exits inside <code>axvm</code>.</p>
<p>That is, providing a <code>run_vcpu()</code> function in VM structure, and consolidate the device emulation-related VM-exit handling into <code>vm.run_vcpu()</code>.</p>
<pre><code class="language-Rust">impl&lt;H: AxVMHal&gt; AxVM&lt;H&gt; {
    pub fn run_vcpu(&amp;self, vcpu_id: usize) -&gt; AxResult&lt;AxVCpuExitReason&gt; {
        let vcpu = self
            .vcpu(vcpu_id)
            .ok_or_else(|| ax_err_type!(InvalidInput, "Invalid vcpu_id"))?;

        vcpu.bind()?;

        let exit_reason = loop {
            let exit_reason = vcpu.run()?;

            trace!("{exit_reason:#x?}");
            let handled = match &amp;exit_reason {
                AxVCpuExitReason::MmioRead { addr: _, width: _ } =&gt; true,
                AxVCpuExitReason::MmioWrite {
                    addr: _,
                    width: _,
                    data: _,
                } =&gt; true,
                AxVCpuExitReason::IoRead { port: _, width: _ } =&gt; true,
                AxVCpuExitReason::IoWrite {
                    port: _,
                    width: _,
                    data: _,
                } =&gt; true,
                AxVCpuExitReason::NestedPageFault { addr, access_flags } =&gt; self
                    .inner_mut
                    .address_space
                    .lock()
                    .handle_page_fault(*addr, *access_flags),
                _ =&gt; false,
            };
            if !handled {
                break exit_reason;
            }
        };

        vcpu.unbind()?;
        Ok(exit_reason)
    }
}
</code></pre>
<p>Thus, consolidate the device emulation operations into the <code>axvm</code> module, so that the <code>vmm-app</code> only needs to pass in configuration files to create emulated device instances as needed, without having to be concerned with the specific runtime behavior of the emulated devices, as well as the address space.</p>
<p>Of course, this is on the condition that these VM-Exits do not trigger the scheduling of the vCPU.</p>
<h2 id="outer-vmvmm-app-handling"><a class="header" href="#outer-vmvmm-app-handling">(Outer-VM)vmm-app handling</a></h2>
<p>We reuse <code>axtask</code> to implement runtime management and scheduling of vCPUs.</p>
<p>This logic is implemented in the vmm-app because the VMM naturally needs to be concerned with vCPU scheduling, and it consolidates the dependency on ArceOS's <code>axtask</code> within the <code>vmm-app</code>.</p>
<p>For VM-Exits that were not handled by the previous two layers, they will be get from the return value of <code>vcpu::run()</code> and processed here, including the handling of hypercalls (handling this within the VMM also seems quite reasonable) and any (if-any) VM-Exit types that require vCPU scheduling or vCPU exit.</p>
<pre><code class="language-Rust">        let mut task = TaskInner::new(
            || {
                let curr = axtask::current();

                let vm = curr.task_ext().vm.clone();
                let vcpu = curr.task_ext().vcpu.clone();
                let vm_id = vm.id();
                let vcpu_id = vcpu.id();

                info!("VM[{}] Vcpu[{}] waiting for running", vm.id(), vcpu.id());
                wait_for(vm_id, || vm.running());

                info!("VM[{}] Vcpu[{}] running...", vm.id(), vcpu.id());

                loop {
                    match vm.run_vcpu(vcpu_id) {
                        // match vcpu.run() {
                        Ok(exit_reason) =&gt; match exit_reason {
                            AxVCpuExitReason::Hypercall { nr, args } =&gt; {
                                debug!("Hypercall [{}] args {:x?}", nr, args);
                            }
                            AxVCpuExitReason::FailEntry {
                                hardware_entry_failure_reason,
                            } =&gt; {
                                warn!(
                                    "VM[{}] VCpu[{}] run failed with exit code {}",
                                    vm_id, vcpu_id, hardware_entry_failure_reason
                                );
                            }
                            AxVCpuExitReason::ExternalInterrupt { vector } =&gt; {
                                debug!("VM[{}] run VCpu[{}] get irq {}", vm_id, vcpu_id, vector);
                            }
                            AxVCpuExitReason::Halt =&gt; {
                                debug!("VM[{}] run VCpu[{}] Halt", vm_id, vcpu_id);
                                wait(vm_id)
                            }
                            AxVCpuExitReason::Nothing =&gt; {}
                            _ =&gt; {
                                warn!("Unhandled VM-Exit");
                            }
                        },
                        Err(err) =&gt; {
                            warn!("VM[{}] run VCpu[{}] get error {:?}", vm_id, vcpu_id, err);
                            wait(vm_id)
                        }
                    }
                }
            },
            format!("VCpu[{}]", vcpu.id()),
            KERNEL_STACK_SIZE,
        );
</code></pre>
<h2 id="end"><a class="header" href="#end">End</a></h2>
<p>Now this is only a draft.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="axvisor-design-discussions"><a class="header" href="#axvisor-design-discussions">AxVisor Design <a href="https://github.com/orgs/arceos-hypervisor/discussions">Discussions</a></a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="editor.js"></script>
        <script src="mode-rust.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="mermaid.min.js"></script>
        <script src="mermaid-init.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
